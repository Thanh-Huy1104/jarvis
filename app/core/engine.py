import logging
import time
from typing import Literal
from functools import partial

# LangGraph & LangChain Imports
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver

# Internal Imports
from app.core.state import AgentState
from app.core.router import JarvisRouter
from app.core.skills import SkillLibrary
from app.core import nodes
from app.core.utils import log_timing_report
from app.adapters.llm_vllm import VllmAdapter
from app.adapters.memory_mem0 import Mem0Adapter
from app.execution.sandbox import DockerSandbox

logger = logging.getLogger(__name__)

class JarvisEngine:
    """
    The main Jarvis orchestration engine.
    
    Architecture:
    1. Router (Intent Classification)
    2. Speed Mode OR Complex Mode
    3. If Complex: Context Builder -> Parallel Planner
    4. Parallel Planner -> Sequential Path OR Parallel Path
    5. Execution & Aggregation
    """
    
    def __init__(self, callbacks=None):
        # Initialize Adapters
        self.router = JarvisRouter()
        self.llm = VllmAdapter()
        self.memory = Mem0Adapter()
        self.skills = SkillLibrary()
        self.sandbox = DockerSandbox()
        
        # Runtime State
        self._timing = {}
        
        # Persistence (In-Memory Checkpointer)
        # Must be initialized once to retain history across graph runs
        self.checkpointer = MemorySaver()
        
        logger.info("JarvisEngine initialized with Modular Architecture")

    # =========================================================================
    # ROUTING LOGIC (The "Brain" - Decides where to go)
    # =========================================================================

    def route_query_node(self, state: AgentState) -> dict:
        """Entry Node: Classifies intent (Speed vs Complex)."""
        start_time = time.time()
        
        # Build minimal context for router
        messages = state.get("messages", [])
        context = ""
        if messages:
            recent = messages[-4:]
            context = "\n".join([f"{m.type}: {m.content[:150]}" for m in recent])
        
        intent = self.router.classify(state["user_input"], conversation_context=context)
        
        self._timing['route_query'] = (time.time() - start_time) * 1000
        return {"intent_mode": intent}

    def determine_intent(self, state: AgentState) -> Literal["speed_agent", "context_builder"]:
        """Conditional Edge: Speed vs Thinking Mode"""
        return "speed_agent" if state["intent_mode"] == "speed" else "context_builder"

    def determine_parallelism(self, state: AgentState) -> Literal["parallel_executor", "think_agent"]:
        """
        Conditional Edge: Decides if we execute sequentially or in parallel 
        based on the plan generated by the planner node.
        """
        plan = state.get("plan", [])
        
        # If plan has > 1 subtasks, go parallel. Otherwise, sequential thinking.
        if plan and len(plan) > 1:
            logger.info(f"→ Routing to PARALLEL execution ({len(plan)} tasks)")
            return "parallel_executor"
        
        logger.info("→ Routing to SEQUENTIAL execution")
        return "think_agent"

    def check_execution_result(self, state: AgentState) -> Literal["think_agent", "skill_proposer"]:
        """
        Conditional Edge: Implements self-correction loop.
        If execution failed, route back to think_agent with error feedback.
        If succeeded, proceed to skill_proposer.
        """
        execution_error = state.get("execution_error")
        retry_count = state.get("retry_count", 0)
        max_retries = 3  # Prevent infinite loops
        
        if execution_error and retry_count < max_retries:
            logger.warning(f"⚠️  Execution failed (attempt {retry_count + 1}/{max_retries}), routing back to think_agent")
            logger.warning(f"Error: {execution_error[:200]}...")
            return "think_agent"
        
        if retry_count >= max_retries:
            logger.error(f"❌ Max retries ({max_retries}) reached, proceeding to skill_proposer")
        else:
            logger.info("✅ Execution succeeded, proceeding to skill_proposer")
        
        return "skill_proposer"

    # =========================================================================
    # GRAPH CONSTRUCTION
    # =========================================================================

    def build(self):
        """Builds the LangGraph StateGraph."""
        workflow = StateGraph(AgentState)
        
        # --- Add Nodes ---
        # Use functools.partial to bind 'self' to node functions
        
        # 1. Classification
        workflow.add_node("router", self.route_query_node)
        
        # 2a. Speed Path (async)
        workflow.add_node("speed_agent", partial(nodes.speed_response, self))
        
        # 2b. Complex Path (Context -> Plan) (async)
        workflow.add_node("context_builder", partial(nodes.build_context, self))
        workflow.add_node("parallel_planner", partial(nodes.plan_parallel_tasks, self))
        
        # 3a. Sequential Path (Think -> Execute with Self-Correction Loop)
        workflow.add_node("think_agent", partial(nodes.reason_and_code, self))
        workflow.add_node("executor", partial(nodes.execute_code, self))
        
        # 3b. Parallel Path (Execute Workers & Aggregate) (async)
        # Note: aggregate_parallel_results handles the concurrent worker execution internally
        workflow.add_node("parallel_executor", partial(nodes.aggregate_parallel_results, self))
        
        # 4. Skill Proposal
        workflow.add_node("skill_proposer", partial(nodes.propose_pending_skill, self))

        # --- Add Edges ---
        
        # Start -> Router
        workflow.add_edge(START, "router")
        
        # Router -> Split (Speed vs Complex)
        workflow.add_conditional_edges(
            "router",
            self.determine_intent,
            {
                "speed_agent": "speed_agent",
                "context_builder": "context_builder"
            }
        )
        
        # Complex Path: Context -> Planner
        workflow.add_edge("context_builder", "parallel_planner")
        
        # Planner -> Split (Parallel vs Sequential)
        workflow.add_conditional_edges(
            "parallel_planner",
            self.determine_parallelism,
            {
                "parallel_executor": "parallel_executor",
                "think_agent": "think_agent"
            }
        )
        
        # Sequential Path Logic: Think -> Execute (with self-correction loop)
        workflow.add_edge("think_agent", "executor")
        
        # Self-Correction Loop: Executor checks result and may loop back to Think
        workflow.add_conditional_edges(
            "executor",
            self.check_execution_result,
            {
                "think_agent": "think_agent",  # Retry with error feedback
                "skill_proposer": "skill_proposer"      # Success or max retries
            }
        )
        
        # Terminal nodes
        workflow.add_edge("skill_proposer", END)
        
        # Parallel Outcome
        workflow.add_edge("parallel_executor", END)
        
        # Speed Outcome
        workflow.add_edge("speed_agent", END)

        # Compile with persistent checkpointer
        return workflow.compile(checkpointer=self.checkpointer)

    # =========================================================================
    # UTILITIES
    # =========================================================================

    async def cleanup(self):
        """Cleanup resources."""
        pass
    
    def report_timing(self):
        """Log execution timing and reset."""
        log_timing_report(self._timing)
        self._timing = {}  # Reset for next request